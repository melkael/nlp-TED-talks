{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocess\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = preprocess.download_and_extract_ted()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = preprocess.preprocess_ted(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4474850 tokens in the dataset.\n",
      "There are 18438 tokens that appear only once.\n",
      "There are 18538 unique tokens to remove.\n",
      "It took 0.4788999557495117 seconds to remove all unnecessary items.\n",
      "There are now only 1926086 tokens in the dataset.\n"
     ]
    }
   ],
   "source": [
    "input_texts = preprocess.clean_tokens_ted(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now only 1924 inputs left.\n"
     ]
    }
   ],
   "source": [
    "#remove all inputs that have less than 500 tokens in them\n",
    "input_texts, labels = preprocess.remove_short_texts(input_texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = preprocess.pad_texts(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, label_lookup = preprocess.preprocess_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, input_indices_list = preprocess.compute_indexes(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.local/lib/python3.5/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('/home/maxime/Documents/nlp/practical_2_bis/glove.txt')\n",
    "tmp_file = get_tmpfile(\"word2vec.txt\")\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "glove = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "glove_vectors = glove.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 35562 words\n",
      "found 34558 word vectors, 0.9717676171193971 of our vocabulary\n",
      "missing words e.g. ['yementimes', 'unsignalized', 'compliances', 'sistas', 'mouaz', 'paraorchestra', 'kelps', 'neurotypical', 'futureless', 'biogenerative', 'flosses', 'sietas', 'myesha', 'britlin', 'isil', 'legadema', 'taimina', 'superbetter', 'patternicity', 'redbrigade', 'rocketcam', 'decellularized', 'templated', 'intiwatana', 'feki', 'retweeted', 'aremeyaw', 'novich', 'vishna', 'romotive', 'eppasod', 'animaris', 'blicket', 'otwoma', 'mahabuba', 'sarcosuchus', 'hyperconnectivity', 'gymnosophist', 'bidialectal', 'dracorex', 'rrrrrrr', 'kaesava', 'impatients', 'solidariot', 'monogamously', 'siyathemba', 'capric', 'terrapower', 'schizophonia', 'kleptoparasites']\n",
      "(35562, 50)\n"
     ]
    }
   ],
   "source": [
    "embeddings = preprocess.clean_vocabulary(word_to_index, glove)\n",
    "print (np.shape(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1540, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "inputs_train, inputs_test, inputs_cv = preprocess.generate_datasets(input_indices_list, labels, label_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(np.shape(embeddings)[0], np.shape(embeddings)[1])\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embeddings))\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.dropout1 = nn.Dropout(.5)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(50, 8)\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x)#.view((1, -1))\n",
    "        embeds = torch.mean(embeds, 1)\n",
    "        out = self.fc1(embeds)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.tanh1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.logsoftmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, mode):\n",
    "    if mode == 'training':\n",
    "        batch_size = 1540\n",
    "        dataset = inputs_train\n",
    "        \n",
    "    elif mode == 'validation':\n",
    "        batch_size = 192\n",
    "        dataset = inputs_cv\n",
    "    else:\n",
    "        batch_size = 192\n",
    "        dataset = inputs_test\n",
    "          \n",
    "    \n",
    "    batch = preprocess.get_data_batch(0, batch_size, dataset)\n",
    "    text = torch.from_numpy(batch[0]).long()\n",
    "    labels = torch.from_numpy(batch[1]).long()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(text)\n",
    "        loss = criterion(outputs, labels)\n",
    "    y_hat = torch.argmax(outputs, dim = 1)\n",
    "    \n",
    "    score = 0\n",
    "    for i in range (y_hat.size()[0]):\n",
    "        if y_hat[i].item() == labels[i]:\n",
    "            score += 1\n",
    "    return score / len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.local/lib/python3.5/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Mean Loss 0.0000, Training score 0.1331, Validation score 0.1094\n",
      "Epoch [2/100], Mean Loss 1.7073, Training score 0.5571, Validation score 0.5573\n",
      "Epoch [3/100], Mean Loss 1.5412, Training score 0.5571, Validation score 0.5573\n",
      "Epoch [4/100], Mean Loss 1.4653, Training score 0.5571, Validation score 0.5573\n",
      "Epoch [5/100], Mean Loss 1.4149, Training score 0.5682, Validation score 0.5781\n",
      "Epoch [6/100], Mean Loss 1.3731, Training score 0.5903, Validation score 0.5885\n",
      "Epoch [7/100], Mean Loss 1.3346, Training score 0.6130, Validation score 0.5990\n",
      "Epoch [8/100], Mean Loss 1.2983, Training score 0.6273, Validation score 0.6042\n",
      "Epoch [9/100], Mean Loss 1.2638, Training score 0.6403, Validation score 0.6146\n",
      "Epoch [10/100], Mean Loss 1.2310, Training score 0.6571, Validation score 0.6250\n",
      "Epoch [11/100], Mean Loss 1.1996, Training score 0.6792, Validation score 0.6198\n",
      "Epoch [12/100], Mean Loss 1.1695, Training score 0.7019, Validation score 0.6250\n",
      "Epoch [13/100], Mean Loss 1.1405, Training score 0.7156, Validation score 0.6354\n",
      "Epoch [14/100], Mean Loss 1.1124, Training score 0.7351, Validation score 0.6302\n",
      "Epoch [15/100], Mean Loss 1.0850, Training score 0.7494, Validation score 0.6458\n",
      "Epoch [16/100], Mean Loss 1.0582, Training score 0.7675, Validation score 0.6406\n",
      "Epoch [17/100], Mean Loss 1.0320, Training score 0.7883, Validation score 0.6406\n",
      "Epoch [18/100], Mean Loss 1.0061, Training score 0.8084, Validation score 0.6458\n",
      "Epoch [19/100], Mean Loss 0.9808, Training score 0.8279, Validation score 0.6458\n",
      "Epoch [20/100], Mean Loss 0.9558, Training score 0.8409, Validation score 0.6458\n",
      "Epoch [21/100], Mean Loss 0.9314, Training score 0.8636, Validation score 0.6510\n",
      "Epoch [22/100], Mean Loss 0.9075, Training score 0.8760, Validation score 0.6615\n",
      "Epoch [23/100], Mean Loss 0.8842, Training score 0.8903, Validation score 0.6719\n",
      "Epoch [24/100], Mean Loss 0.8615, Training score 0.8994, Validation score 0.6719\n",
      "Epoch [25/100], Mean Loss 0.8395, Training score 0.9104, Validation score 0.6667\n",
      "Epoch [26/100], Mean Loss 0.8182, Training score 0.9182, Validation score 0.6667\n",
      "Epoch [27/100], Mean Loss 0.7976, Training score 0.9286, Validation score 0.6615\n",
      "Epoch [28/100], Mean Loss 0.7776, Training score 0.9351, Validation score 0.6667\n",
      "Epoch [29/100], Mean Loss 0.7584, Training score 0.9409, Validation score 0.6667\n",
      "Epoch [30/100], Mean Loss 0.7398, Training score 0.9448, Validation score 0.6615\n",
      "Epoch [31/100], Mean Loss 0.7218, Training score 0.9532, Validation score 0.6562\n",
      "Epoch [32/100], Mean Loss 0.7046, Training score 0.9591, Validation score 0.6562\n",
      "Epoch [33/100], Mean Loss 0.6879, Training score 0.9656, Validation score 0.6615\n",
      "Epoch [34/100], Mean Loss 0.6718, Training score 0.9708, Validation score 0.6562\n",
      "Epoch [35/100], Mean Loss 0.6563, Training score 0.9747, Validation score 0.6562\n",
      "Epoch [36/100], Mean Loss 0.6414, Training score 0.9766, Validation score 0.6510\n",
      "Epoch [37/100], Mean Loss 0.6270, Training score 0.9799, Validation score 0.6510\n",
      "Epoch [38/100], Mean Loss 0.6130, Training score 0.9831, Validation score 0.6510\n",
      "Epoch [39/100], Mean Loss 0.5996, Training score 0.9851, Validation score 0.6510\n",
      "Epoch [40/100], Mean Loss 0.5867, Training score 0.9864, Validation score 0.6458\n",
      "Epoch [41/100], Mean Loss 0.5741, Training score 0.9903, Validation score 0.6406\n",
      "Epoch [42/100], Mean Loss 0.5621, Training score 0.9922, Validation score 0.6406\n",
      "Epoch [43/100], Mean Loss 0.5504, Training score 0.9929, Validation score 0.6406\n",
      "Epoch [44/100], Mean Loss 0.5391, Training score 0.9942, Validation score 0.6406\n",
      "Epoch [45/100], Mean Loss 0.5282, Training score 0.9968, Validation score 0.6406\n",
      "Epoch [46/100], Mean Loss 0.5177, Training score 0.9974, Validation score 0.6354\n",
      "Epoch [47/100], Mean Loss 0.5075, Training score 0.9987, Validation score 0.6354\n",
      "Epoch [48/100], Mean Loss 0.4977, Training score 0.9994, Validation score 0.6354\n",
      "Epoch [49/100], Mean Loss 0.4882, Training score 1.0000, Validation score 0.6354\n",
      "Epoch [50/100], Mean Loss 0.4790, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [51/100], Mean Loss 0.4702, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [52/100], Mean Loss 0.4616, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [53/100], Mean Loss 0.4533, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [54/100], Mean Loss 0.4453, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [55/100], Mean Loss 0.4375, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [56/100], Mean Loss 0.4300, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [57/100], Mean Loss 0.4227, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [58/100], Mean Loss 0.4156, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [59/100], Mean Loss 0.4088, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [60/100], Mean Loss 0.4022, Training score 1.0000, Validation score 0.6302\n",
      "Epoch [61/100], Mean Loss 0.3957, Training score 1.0000, Validation score 0.6354\n",
      "Epoch [62/100], Mean Loss 0.3895, Training score 1.0000, Validation score 0.6354\n",
      "Epoch [63/100], Mean Loss 0.3835, Training score 1.0000, Validation score 0.6354\n",
      "Epoch [64/100], Mean Loss 0.3776, Training score 1.0000, Validation score 0.6354\n",
      "Epoch [65/100], Mean Loss 0.3719, Training score 1.0000, Validation score 0.6354\n",
      "Epoch [66/100], Mean Loss 0.3664, Training score 1.0000, Validation score 0.6406\n",
      "Epoch [67/100], Mean Loss 0.3610, Training score 1.0000, Validation score 0.6406\n",
      "Epoch [68/100], Mean Loss 0.3558, Training score 1.0000, Validation score 0.6406\n",
      "Epoch [69/100], Mean Loss 0.3507, Training score 1.0000, Validation score 0.6406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-60ae4198c862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "losses = []\n",
    "mean_loss = 0\n",
    "\n",
    "model = Classifier(50).double()\n",
    "model.train()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    i = 0\n",
    "    print('Epoch [{}/{}], Mean Loss {:.4f}, Training score {:.4f}, Validation score {:.4f}'\n",
    "          .format(epoch+1, num_epochs, mean_loss, \n",
    "                  evaluate_model(model, 'training'), evaluate_model(model, 'validation')))\n",
    "    \n",
    "    \n",
    "    while 'computing the dataset by 44-size minibatches': # 1540 % 44 = 0 and im lazy krkr  \n",
    "        if(i > 1539):\n",
    "            i = 0\n",
    "            break\n",
    "        else:\n",
    "            mini_batch = preprocess.get_data_batch(i, 44, inputs_train)\n",
    "            text = torch.from_numpy(mini_batch[0]).long()\n",
    "            labels = torch.from_numpy(mini_batch[1]).long()\n",
    "            \n",
    "        # Forward pass\n",
    "        outputs = model(text)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss)\n",
    "        mean_loss = sum(losses) / len (losses)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        i += 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.local/lib/python3.5/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6197916666666666"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
